
!======================================================================C
!                                                                      C
! SOFTWARE NAME : FRONTFLOW_BLUE.1.0                                   C
!                                                                      C
!  SUB ROUTINE    DD_MPI                                               C
!                                                                      C
!                                       WRITTEN BY C.KATO              C
!                                                                      C
!                                                                      C
! Contact address: The University of Tokyo, FSIS project               C  
!                                                                      C
!======================================================================C
!
!      GENERIC TO MPI FORTRAN INTERFACE FOR DOMAIN-DECOMPOSITION
!     PROGRAMMING MODEL
!                        AUTHOR: C. KATO, MERL, HITACHI, LTD.
!                        DATE FIRST WRITTEN : FEBRUARY 20TH, 1998 
!                        DATE       MODIFIED: APRIL    14TH, 2001
!                            (-PRECEXP OPTION IMPLEMENTED)
!                        DATE LAST  MODIFIED: MARCH     7TH, 2003
!                            (ENTRIES DDCOM1 AND DDCOM2 ADDED)
!
!
      SUBROUTINE DDINIT(NPART,IPART)
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!
!
!      INITIALIZE A PARALLEL TASK FOR DOMAIN-DECOMPOSITION PROGRAMMING
!     MODEL
!                            ( MPI VERSION )
!
!
! NOTE 1; THIS SUBROUTINE QUERIES THE TASK NUMBER (I.E. TASK ID) OF THE
!       CALLING TASK, THE NUMBER OF TASKS WHICH ARE SHARING THE PROGRAM
!       RUN. THE NUMBER OF SUB-DOMAINS AND THE SUB-DOMAIN NUMBER
!       THAT THE CALLING TASK SHOULD TAKE CARE OF, WILL BE RETURNED TO
!       THE CALLING TASK, AFTER BEING SET TO THE NUMBER OF TASKS, AND
!       THE TASK NUMBER PLUS ONE, RESPECTIVELY.
!
! NOTE 2; THE NUMBER OF TASKS TO SHARE A PARTICULAR RUN IS DETERMINED AT
!       RUN TIME BY THE PARALLEL ENVIRONMENT, WHILE THE NUMBER OF
!       SUB-DOMAINS IS THE SAME AS THE NUMBER OF SUB-DOMAIN FILES WHICH
!       HAVE BEEN PREPARED BEFORE THE RUN. THEREFORE, IN SOME CASES, 
!       THE NUMBER OF TASKS MIGHT DIFFER FROM THE ACTUAL NUMBER OF
!       SUB-DOMAINS, ALTHOUGH THEY MUST BE THE SAME TO RUN A MEANINGFUL
!       COMPUTATION. NOTE THAT THIS SUBROUTINE DOES NOT CHECK THE 
!       CONSISTENCY OF THESE VALUES.
!
! NOTE 3; A TASK NUMBER (I.E. TASK ID) IS A UNIQUE NUMBER FROM 0 TO ONE
!       MINUS THE TOTAL NUMBER OF TASKS, ASSIGNED BY THE SYSTEM AT RUN
!       TIME. THUS, THE SUB-DOMAIN NUMBER IS ALSO A UNIQUE NUMBER 
!       FROM 1 TO THE NUMBER OF SUB-DOMAINS.
!
! NOTE 4; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
!          ( NONE )
!
!       (2) OUTPUT
! INT *4   NPART       ; TOTAL NUMBER OF SUB-DOMAINS
! INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK SHOULD
!                       TAKE CARE OF
!
!
      logical flag
!      call MPI_Initialized(flag, IERR)
!      if (.not. flag) call MPI_Init(IERR)
!*    CALL MPI_INIT(IERR)
      CALL MPI_COMM_SIZE(MPI_COMM_WORLD,NTASK,IERR)
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!
!      IF (ITASK.eq.0) PRINT *,"CALL DDINIT"
      NPART = NTASK
      IPART = ITASK+1
!
!
      RETURN
      END
      SUBROUTINE DDEXIT
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!
!
!      EXIT FROM PARALLEL EXECUTIONS
!                            ( MPI VERSION )
!
!
! NOTE ; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
!          ( NONE )
!
!       (2) OUTPUT
!          ( NONE )
!
!
!      CALL MPI_FINALIZE(IERR)
!
!
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL DDEXIT"
      RETURN
      END
      SUBROUTINE DDSYNC
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!
!
!      IMPLEMENT BARRIER SYNCHRONIZATION AMONG THE GROUP OF ALL TASKS
!                            ( MPI VERSION )
!
!
! NOTE ; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
!          ( NONE )
!
!       (2) OUTPUT
!          ( NONE )
!
!
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL DDSYNC"
      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
!
!
      RETURN
      END
!
!
      SUBROUTINE DDSTOP(IPART,IUT0)
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!
      CHARACTER*60 ERMSGB  / ' ## SUBROUTINE DDSTOP: FATAL      ERROR REPORTED ; STOPPING' /
      CHARACTER*60 EREXP1  / ' A SUB-DOMAIN COMPUTATION HAS BEEN ABNORMALLY TERMINATED AT' /
!
!
!      STOP ALL THE RUNNING PARALLEL TASKS FOR DOMAIN-DECOMPOSITION
!     PROGRAMMING MODEL
!                            ( MPI VERSION )
!
!
! NOTE 1; IF AN ERROR CONDITION HAS BEEN DETECTED IN SOME TASK RUNNING
!       IN PARALLEL, ALL THE TASKS SHARING THAT PARTICULAR RUN SHOULD BE
!       APPROPRIATELY STOPPED. THIS SUBROUTINE TERMINATES ALL THE 
!       RUNNING TASKS AND CANCELS THE PARALLEL JOB.
!
! NOTE 2; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK IS
!                       TAKING CARE OF
! INT *4   IUT0        ; FILE NUMBER TO WRITE ERROR MESSAGE
!
!       (2) OUTPUT
!          ( NONE )
!
!
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL DDSTOP"
      WRITE(IUT0,*) ERMSGB
      WRITE(IUT0,*) EREXP1, IPART
      CALL MPI_ABORT(MPI_COMM_WORLD,IPART,IERR)
!
      RETURN
      END
!
!
      SUBROUTINE DDCOM0(LPINT1,LPINT2,LPINT3,NPINT,MDOM,MBPDOM,LDOM,NBPDOM,NDOM,IPSLF,IPSND,IUT0,IERR)
      IMPLICIT REAL*4(A-H,O-Z)
      DIMENSION LPINT1(NPINT),LPINT2(NPINT),LPINT3(NPINT),LDOM(MDOM),NBPDOM(MDOM),IPSLF(MBPDOM,MDOM),IPSND(MBPDOM,MDOM)
!
!
      CHARACTER*60 ERMSGB / ' ## SUBROUTINE DDCOM0: FATAL     ERROR OCCURRENCE; RETURNED' /
      CHARACTER*60 EREXP1 / ' DIMENSION SIZE OF PASSED LIST ARRAYS IS NOT SUFFICIENT    ' /
!
!
!      SET UP NEIGHBORING DOMAIN LISTS FOR DOMAIN-DECOMPOSITION
!     PROGRAMMING MODEL
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! INT *4   LPINT1 (IBP)     ; INTER-CONNECT BOUNDARY NODES
! INT *4   LPINT2 (IBP)     ; NEIGHBORING SUB-DOMAIN NUMBERS
! INT *4   LPINT3 (IBP)     ; NODE NUMBER IN THE NEIGHBORING SUB-DOMAINS
! INT *4   NPINT            ; NUMBER OF INTER-CONNECT BOUNDARY NODES
!
! INT *4   MDOM             ; MAX. NUMBER OF THE NEIGHBORING SUB-DOMAINS
! INT *4   MBPDOM           ; THE DIMENSION SIZE OF THE FIRST ELEMENTS
!                            OF THE PASSED ARRAYS 'IPSLF' AND 'IPSND'
!                            (I.E. THE MAXIMUM NUMBER OF THE
!                             INTER-CONNECT BOUNDARY NODES FOR A
!                             NEIGHBORING SUB-DOMAIN)
! INT *4   IUT0             ; FILE NUMBER TO WRITE ERROR MESSAGE
!
!       (2) OUTPUT
! INT *4   LDOM      (IDOM) ; NEIGHBORING SUB-DOMAIN NUMBER
! INT *4   NBPDOM    (IDOM) ; NUMBER OF INTER-CONNECT BOUNDARY NODES
!                            SHARING WITH THE IDOM'TH NEIGHBORING
!                            SUB-DOMAIN, LDOM(IDOM)
! INT *4   NDOM             ; NUMBER OF THE NEIGHBORING SUB-DOMAINS
! INT *4   IPSLF (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            CALLING TASK'S SUB-DOMAIN, FOR THE IDOM'TH
!                            NEIGHBORING SUB-DOMAIN, LDOM(IDOM)
! INT *4   IPSND (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            SUB-DOMAIN THAT IS RECEIVING THE CALLING
!                            TASK'S RESIDUALS.
! INT *4   IERR             ; RETURN CODE WHOSE VALUE WILL BE EITHER
!                   0 --- INDICATING SUCCESSFUL TERMINATION
!                OR 1 --- INDICATING OCCURRENCE OF SOME ERROR CONDITIONS
!
!
!      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL DDCOM0"
      IERR = 0
!
!
!
! SET INITIAL VALUES
!
!
!
      NDOM = 0
      DO 100 IDOM = 1 , MDOM
          NBPDOM(IDOM) = 0
  100 CONTINUE
!
!
!
! SET NEIGHBORING DOMAIN LISTS
!
!
!
      DO 230 IPINT = 1 , NPINT
          IFNEW = LPINT2(IPINT)
          DO 210 ICHK = 1 , NDOM
              IF(LDOM(ICHK).EQ.IFNEW) THEN
                  IDOM = ICHK
                  GO TO 220
              ENDIF
  210     CONTINUE
          NDOM = NDOM+1
          IDOM = NDOM
!
          IF(NDOM.GT.MDOM) THEN
              WRITE(IUT0,*) ERMSGB
              WRITE(IUT0,*) EREXP1
              IERR = 1
              RETURN
          ENDIF
!
          LDOM(NDOM) = IFNEW
!
  220     CONTINUE
          NBPDOM(IDOM) = NBPDOM(IDOM)+1
!
          IF(NBPDOM(IDOM).GT.MBPDOM) THEN
              WRITE(IUT0,*) ERMSGB
              WRITE(IUT0,*) EREXP1
              IERR = 1
              RETURN
          ENDIF
!
          IPSLF(NBPDOM(IDOM),IDOM) = LPINT1(IPINT)
          IPSND(NBPDOM(IDOM),IDOM) = LPINT3(IPINT)
  230 CONTINUE
!
!
      RETURN
      END
!
!
      SUBROUTINE DDCOM1(LPINT1,NPINT,NUMIP,NP,IUT0,IERR)
      IMPLICIT REAL*4(A-H,O-Z)
      DIMENSION LPINT1(NPINT),NUMIP(NP)
!
!
      CHARACTER*60 ERMSGB / ' ## SUBROUTINE DDCOM1: FATAL     ERROR OCCURRENCE; RETURNED' /
      CHARACTER*60 EREXP1 / ' SPECIFIED NODE NUMBER IS OUT OF RANGE                     ' /
!
!
!      CALCULATE NUMBER OF NEIGHBORING DOMAINS THAT EACH NODE BELONGS TO
!     FOR DOMAIN-DECOMPOSITION PROGRAMMING MODEL
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! INT *4   LPINT1 (IBP)     ; INTER-CONNECT BOUNDARY NODES
! INT *4   NPINT            ; NUMBER OF INTER-CONNECT BOUNDARY NODES
! INT *4   NP               ; NUMBER OF TOTAL    NODES
! INT *4   IUT0             ; FILE NUMBER TO WRITE ERROR MESSAGE
!
!       (2) OUTPUT
! INT *4   NUMIP  (IP)      ; NUMBER OF NEIGHBORING DOMAINS THAT NODE
!                            'IP' BELONG TO
! INT *4   IERR             ; RETURN CODE WHOSE VALUE WILL BE EITHER
!                   0 --- INDICATING SUCCESSFUL TERMINATION
!                OR 1 --- INDICATING OCCURRENCE OF SOME ERROR CONDITIONS
!
!

      IERR = 0
!
!
!
! SET INITIAL VALUES
!
!
!
      DO 100 IP = 1 , NP
          NUMIP(IP) = 0
  100 CONTINUE
!
!
!
! CALCULATE NUMBER OF NEIGHBORING DOMAINS THAT NODE 'IP' BELONGS TO
!
!
!
      DO 110 IPINT = 1 , NPINT
          IP = LPINT1(IPINT)
!
          IF(IP.LT.1 .OR. IP.GT.NP) THEN
              WRITE(IUT0,*) ERMSGB
              WRITE(IUT0,*) EREXP1
              IERR = 1
              RETURN
          ENDIF
!
          NUMIP(IP) = NUMIP(IP)+1
  110 CONTINUE
!
!
      RETURN
      END
!
!
      SUBROUTINE DDCOM2(SEND,RECV)
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!      INCLUDE 'xmp_coarray.h'
!C$XMP NODES PDDCOM2(*)
!
!
!      SUM UP A SINGLE SCALAR AMONG ALL THE PARTICIPATING TASKS FOR
!     DOMAIN-DECOMPOSITION PROGRAMMING MODEL
!
!                            ( MPI VERSION )
!
!
! NOTE 1; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
! NOTE 2; SOME COMPILERS, SUCH AS OFORT90 IN HI-UXMPP, SUPPORT AUTOMATIC
!       PRECISION EXPANSION, WHERE ALL THE CONSTANTS, VARIABLES AND
!       ARRAYS OF 4-BYTE PRECISION (REAL*4) ARE AUTOMATICALLY CONVERTED
!       TO THOSE OF 8-BYTE PRECISION (REAL*8) WITH UNFORMATTED I/O DATA 
!       BEING KEPT AS THEY ARE (IF SO SPECIFIED). WHEN USING SUCH 
!       FEATURES (FUNCTIONS) OF A COMPILER, SPECIAL CARE IS NEEDED
!       BECAUSE A COUPLE OF MPI SUBROUTINES CALLED IN THIS SUBPROGRAM
!       ACCEPT THE DATA TYPE (DATA PRECISION) AS THEIR INPUT AND
!       PERFORM THE OPERATIONS ACCORDING TO THIS INPUT VALUE. THIS
!       INTERFACE SUPPORTS THE AUTOMATIC PRECISION EXPANSION MENTIONED
!       ABOVE. IF YOU WISH TO USE SUCH FEATURE, ADD '-DPRECEXP' OPTION
!       WHEN INVOKING 'cpp' FOR PRI-PROCESSING THIS SOURCE PROGRAM FILE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! REAL*4   SEND             ; SCALAR VARIABLE TO SUM UP
!
!       (2) OUTPUT
! REAL*4   RECV             ; SCALAR VARIABLE SUMMED UP AMONG ALL TASKS
!
!
!      
#ifdef USE_BARRIER
      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
#endif
      
#ifdef PRECEXP
      CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL8,MPI_SUM,MPI_COMM_WORLD,IERR)
#else
      RECV = SEND
! Fujitsu start 202103
      CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL ,MPI_SUM,MPI_COMM_WORLD,IERR)
!      CALL CO_SUM(SEND,RECV)
! Fujitsu end 202103
!C$XMP REDUCTION(+:RECV) on PDDCOM2
!C$XMP BCAST (RECV) FROM PDDCOM2(1)

!      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF(ITASK.EQ.0)PRINT *,"ALLREDUCE=",RECV,"REDUCTION=",RECV2
#endif
      
#ifdef USE_BARRIER
      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
#endif
      
!
      RETURN
      END
!
!CTT SUBROUTINE DDCOM2 END
!
!CTT SUBROUTINE DDALLD BEGIN
!
      SUBROUTINE DDALLD(SEND,RECV,IFLAG,IUT0,IERR)
      IMPLICIT REAL*4(A-H,O-Z)
!
      INCLUDE 'mpif.h'
!
!
      CHARACTER*60 ERMSGB  / ' ## SUBROUTINE DDALLD: FATAL   ERROR OCCURENCE; RETURNED ' /
      CHARACTER*60 ERMSG1  / ' ILLIGAL OPERATION NUMBER IS GIVEN ' /
!
!
!      SUM UP A SINGLE SCALAR AMONG ALL THE PARTICIPATING TASKS FOR
!     DOMAIN-DECOMPOSITION PROGRAMMING MODEL
!
!                            ( MPI VERSION )
!
!
! NOTE 1; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
! NOTE 2; SOME COMPILERS, SUCH AS OFORT90 IN HI-UXMPP, SUPPORT AUTOMATIC
!       PRECISION EXPANSION, WHERE ALL THE CONSTANTS, VARIABLES AND
!       ARRAYS OF 4-BYTE PRECISION (REAL*4) ARE AUTOMATICALLY CONVERTED
!       TO THOSE OF 8-BYTE PRECISION (REAL*8) WITH UNFORMATTED I/O DATA 
!       BEING KEPT AS THEY ARE (IF SO SPECIFIED). WHEN USING SUCH 
!       FEATURES (FUNCTIONS) OF A COMPILER, SPECIAL CARE IS NEEDED
!       BECAUSE A COUPLE OF MPI SUBROUTINES CALLED IN THIS SUBPROGRAM
!       ACCEPT THE DATA TYPE (DATA PRECISION) AS THEIR INPUT AND
!       PERFORM THE OPERATIONS ACCORDING TO THIS INPUT VALUE. THIS
!       INTERFACE SUPPORTS THE AUTOMATIC PRECISION EXPANSION MENTIONED
!       ABOVE. IF YOU WISH TO USE SUCH FEATURE, ADD '-DPRECEXP' OPTION
!       WHEN INVOKING 'cpp' FOR PRI-PROCESSING THIS SOURCE PROGRAM FILE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! REAL*4   SEND             ; SCALAR VARIABLE TO SUM UP
! INTEGER  IFLAG            ; OPERATION DISCRIMINATER
!                             1: OP=MPI_SUM
!                             2: OP=MPI_MAX
!                             3: OP=MPI_MIN
!
!       (2) OUTPUT
! REAL*4   RECV             ; SCALAR VARIABLE SUMMED UP AMONG ALL TASKS
!
!
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL ALLD"
      IF(IFLAG.EQ.1) THEN
!
#ifdef PRECEXP
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL8,MPI_SUM,MPI_COMM_WORLD,IERR)
#else
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL ,MPI_SUM,MPI_COMM_WORLD,IERR)
#endif
!
      ELSE IF (IFLAG.EQ.2) THEN
!
#ifdef PRECEXP
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL8,MPI_MAX,MPI_COMM_WORLD,IERR)
#else
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL ,MPI_MAX,MPI_COMM_WORLD,IERR)
#endif
!          
      ELSE IF (IFLAG.EQ.3) THEN
!
#ifdef PRECEXP
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL8,MPI_MIN,MPI_COMM_WORLD,IERR)
#else
          CALL MPI_ALLREDUCE(SEND,RECV,1,MPI_REAL ,MPI_MIN,MPI_COMM_WORLD,IERR)
#endif
!
      ELSE
          WRITE(IUT0,*) ERMSGB
          WRITE(IUT0,*) ERMSG1
          IERR=1
          RETURN
      ENDIF
!
      RETURN
      END
!
!CTT SUBROUTINE DDALLD END
!
!CTT SUBROUTINE DDCOM3 BEGIN
!
      SUBROUTINE DDCOM3(IPART,IDIM,LDOM,NBPDOM,NDOM,IPSLF,IPSND,MBPDOM,FX,FY,FZ,NP,IUT0,IERR,BUFSND,BUFRCV,MAXBUF)
      IMPLICIT REAL*4(A-H,O-Z)
      DIMENSION LDOM(NDOM),NBPDOM(NDOM),IPSLF(MBPDOM,NDOM),IPSND(MBPDOM,NDOM), FX(NP),FY(NP),FZ(NP),BUFSND(MAXBUF),BUFRCV(MAXBUF)
!
      INCLUDE 'mpif.h'
!
      PARAMETER ( MAXDOM = 10000 )
      INTEGER*4 MSGIDS(MAXDOM),MSGSTS(MPI_STATUS_SIZE,MAXDOM)
!
!
      CHARACTER*60 ERMSGB  / ' ## SUBROUTINE DDCOM3: FATAL     ERROR OCCURRENCE; RETURNED' /
      CHARACTER*60 EREXP1  / ' DIMENSION SIZE OF INTERNAL      ARRAYS IS NOT SUFFICIENT  ' /
      CHARACTER*60 EREXP2  / ' DIMENSION SIZE OF PASSED BUFFER ARRAYS IS NOT SUFFICIENT  ' /
      CHARACTER*60 EREXP3  / ' RECEIVED NODE NUMBER IS OUT OF THE GLOBAL NODE NUMBER     ' /
!
!
!      EXCHANGE X, Y, AND Z RESIDUALS AMONG THE NEIGHBORING SUB-DOMAINS
!     AND SUPERIMPOSE THE EXCHANGED RESIDUALS TO THE CALLING TASK'S
!     RESIDUALS, FOR DOMAIN-DECOMPOSITION PROGRAMMING MODEL
!
!                            ( MPI VERSION )
!
!
! NOTE 1; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
! NOTE 2; SOME COMPILERS, SUCH AS OFORT90 IN HI-UXMPP, SUPPORT AUTOMATIC
!       PRECISION EXPANSION, WHERE ALL THE CONSTANTS, VARIABLES AND
!       ARRAYS OF 4-BYTE PRECISION (REAL*4) ARE AUTOMATICALLY CONVERTED
!       TO THOSE OF 8-BYTE PRECISION (REAL*8) WITH UNFORMATTED I/O DATA 
!       BEING KEPT AS THEY ARE (IF SO SPECIFIED). WHEN USING SUCH 
!       FEATURES (FUNCTIONS) OF A COMPILER, SPECIAL CARE IS NEEDED
!       BECAUSE A COUPLE OF MPI SUBROUTINES CALLED IN THIS SUBPROGRAM
!       ACCEPT THE DATA TYPE (DATA PRECISION) AS THEIR INPUT AND
!       PERFORM THE OPERATIONS ACCORDING TO THIS INPUT VALUE. THIS
!       INTERFACE SUPPORTS THE AUTOMATIC PRECISION EXPANSION MENTIONED
!       ABOVE. IF YOU WISH TO USE SUCH FEATURE, ADD '-DPRECEXP' OPTION
!       WHEN INVOKING 'cpp' FOR PRI-PROCESSING THIS SOURCE PROGRAM FILE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK IS
!                       TAKING CARE OF
!           NOTES ; ARGUMENT 'IPART' IS NOT CURRENTLY USED. IT IS
!                  RETAINED FOR A POSSIBLE FUTURE USE.
! INT *4   IDIM             ; SPACE DIMENSION ( 1, 2, OR 3 )
! INT *4   LDOM      (IDOM) ; NEIGHBORING SUB-DOMAIN NUMBER
! INT *4   NBPDOM    (IDOM) ; NUMBER OF INTER-CONNECT BOUNDARY NODES
!                            SHARING WITH THE IDOM'TH NEIGHBORING
!                            SUB-DOMAIN, LDOM(IDOM)
! INT *4   NDOM             ; NUMBER OF THE NEIGHBORING SUB-DOMAINS
! INT *4   IPSLF (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            CALLING TASK'S SUB-DOMAIN, FOR THE IDOM'TH
!                            NEIGHBORING SUB-DOMAIN, LDOM(IDOM)
! INT *4   IPSND (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            SUB-DOMAIN THAT IS RECEIVING THE CALLING
!                            TASK'S RESIDUALS.
! INT *4   MBPDOM           ; THE DIMENSION SIZE OF THE FIRST ELEMENTS
!                            OF THE PASSED ARRAYS 'IPSLF' AND 'IPSND'
!                            (I.E. THE MAXIMUM NUMBER OF THE
!                             INTER-CONNECT BOUNDARY NODES FOR A
!                             NEIGHBORING SUB-DOMAIN)
! INT *4   NP               ; NUMBER OF THE TOTAL NODES IN THE CALLING
!                            TASK'S SUB-DOMAIN
! INT *4   IUT0             ; FILE NUMBER TO WRITE ERROR MESSAGE
! INT *4   MAXBUF           ; LENGTH OF THE PASSED COMMUNICATION BUFFERS
!                            'BUFSND' AND 'BUFRCV' IN WORDS. 'MAXBUF'
!                             MUST BE NO SMALLER THAN 4 TIMES THE TOTAL
!                             NUMBER OF INTER-CONNECT BOUNDARY NODES IN
!                             THE CALLING TASK
!
!       (2) OUTPUT
! INT *4   IERR             ; RETURN CODE WHOSE VALUE WILL BE EITHER
!                   0 --- INDICATING SUCCESSFUL TERMINATION
!                OR 1 --- INDICATING OCCURRENCE OF SOME ERROR CONDITIONS
!
!       (3) INPUT-OUTPUT
! REAL*4   FX(IP)           ; X-DIRECTION RESIDUAL VECTOR
! REAL*4   FY(IP)           ; Y-DIRECTION RESIDUAL VECTOR
! REAL*4   FZ(IP)           ; Z-DIRECTION RESIDUAL VECTOR
!
!       (4) WORK
! REAL*4   BUFSND(IBUF)     ; HOLDS THE VALUES OF THE INTER-CONNECT
!                            BOUNDARY NODE NUMBER IN THE NEIGHBORING
!                            SUB-DOMAINS AND THE RESIDUALS OF THE
!                            CALLING TASK'S SUB-DOMAIN WHEN SENDING
!                            THE RESIDUALS
!                         
! REAL*4   BUFRCV(IBUF)     ; HOLDS THE VALUES OF THE INTER-CONNECT
!                            BOUNDARY NODE NUMBER IN THE CALLING TASK'S
!                            SUB-DOMAIN AND THE RESIDUALS OF THE
!                            NEIGHBORING SUB-DOMAINS AT THE RECEIPT OF
!                            THE RESIDUALS FROM THE NEIGHBORING
!                            SUB-DOMAINS
!
!
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
!      IF (ITASK.eq.0) PRINT *,"CALL DDCOM3"
      IERR = 0
!
!
!
! CHECK THE INTERNAL ARRAY SIZE
!
!
!
      IF(2*NDOM.GT.MAXDOM) THEN
          WRITE(IUT0,*) ERMSGB
          WRITE(IUT0,*) EREXP1
          IERR = 1
          RETURN
      ENDIF
!
!
!
! POST ALL THE EXPECTED RECEIVES
!
!
!
      NSTART = 1
      DO 110 IDOM = 1 , NDOM
          MSGTYP = 1
          IRECV  = LDOM(IDOM)-1
          MSGLEN = 4*NBPDOM(IDOM)
!
          IF(NSTART+MSGLEN-1.GT.MAXBUF) THEN
              WRITE(IUT0,*) ERMSGB
              WRITE(IUT0,*) EREXP2
              IERR = 1
              RETURN
          ENDIF
!
#ifdef PRECEXP
          CALL MPI_IRECV(BUFRCV(NSTART),MSGLEN,MPI_REAL8,IRECV,MSGTYP,MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
 #else
          CALL MPI_IRECV(BUFRCV(NSTART),MSGLEN,MPI_REAL ,IRECV,MSGTYP,MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
#endif
!
          NSTART = NSTART+MSGLEN
  110 CONTINUE
!
!      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
!
!
!
! SET UP THE SEND BUFFER
!
!
!C    CALL FTRACE_REGION_BEGIN("ddcom3:200-210")
!CDIR PARALLEL DO PRIVATE(NSTART,IP,IPS)
      DO 210 IDOM = 1 , NDOM
          NSTART  = 0
          DO 205 ITMP = 2 , IDOM
              NSTART = NSTART + NBPDOM(ITMP-1)*4
  205     CONTINUE
!CDIR NOINNER
          DO 200 IBP = 1 , NBPDOM(IDOM)
              IP  = IPSLF(IBP,IDOM)
              IPS = IPSND(IBP,IDOM)
              BUFSND(NSTART+1) = IPS
              BUFSND(NSTART+2) = FX(IP)
              BUFSND(NSTART+3) = FY(IP)
              BUFSND(NSTART+4) = FZ(IP)
              NSTART = NSTART + 4
  200     CONTINUE
  210 CONTINUE
!C    CALL FTRACE_REGION_END("ddcom3:200-210")
!
!
!
! SEND THE RESIDUALS
!
!
!
      NSTART = 1
      DO 220 IDOM = 1 , NDOM
          MSGTYP = 1
          ISEND  = LDOM(IDOM)-1
          MSGLEN = 4*NBPDOM(IDOM)

#ifdef PRECEXP
          CALL MPI_ISEND(BUFSND(NSTART),MSGLEN,MPI_REAL8,ISEND,MSGTYP,MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
#else
          CALL MPI_ISEND(BUFSND(NSTART),MSGLEN,MPI_REAL ,ISEND,MSGTYP,MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
#endif
!
          NSTART = NSTART+MSGLEN 
  220 CONTINUE
!
!
!
! WAIT FOR THE COMPLETION OF ALL THE REQUESTED COMMUNICATIONS
!
!
!
      CALL MPI_WAITALL(2*NDOM,MSGIDS,MSGSTS,IERR)
! 
! 
! IMPORTANT NOTES!
!        AFTER A NON-BLOCKING SEND/RECEIVE ROUTINE, SUCH AS 'MPI_ISEND'
!    OR 'MPI_IRECV', IS CALLED, THE COMMUNICATION REQUEST CREATED BY
!    THESE ROUTINES MUST BE FREED EITHER BY EXPLICITLY OR IMPLICITLY.
!   'MPI_REQUEST_FREE' FREES SUCH REQUEST EXPLICITLY, WHILE A ROUTINE
!    WHICH IDENTIFIES COMPLETION OF THE REQUEST, SUCH AS 'MPI_WAIT',
!    'MPI_WAITANY', OR 'MPI_WAITALL' IMPLICITLY FREES THE REQUEST.
!        THIS INTERFACE PROGRAM USES 'MPI_WAITALL' ROUTINES TO FREE SUCH
!    REQUESTS. PAY PARTICULAR ATTENTION IF YOU WISH TO, INSTEAD, USE
!   'MPI_REQUEST_FREE', BECAUSE 'MPI_REQUEST_FREE' FREES THE REQUESTS
!    REGARDLESS OF THE STATE OF THE PREVIOUSLY CALLED COMMUNICATION
!    ROUTINES, THUS SOMETIMES FREES REQUESTS WHICH HAVE NOT BEEN
!    COMPLETED.
!
!
! SUPERIMPOSE THE RECEIVED RESIDUALS
!
!
!C    CALL FTRACE_REGION_BEGIN("ddcom3:300-310")
      NSTARTMP = 0
      DO IDOM = 1 , NDOM
!CDIR NOINNER
        DO IBP = 1, NBPDOM(IDOM)
          IP = BUFRCV(NSTARTMP+1)+0.1
          IF(IP.LT.1 .OR. IP.GT.NP) THEN
            IERR = 1
          ENDIF
          NSTARTMP = NSTARTMP + 4
        ENDDO
      ENDDO
!
      IF(IERR .eq. 1) THEN
        WRITE(IUT0,*) ERMSGB
        WRITE(IUT0,*) EREXP3
        RETURN
      ENDIF
!
      IF(IDIM .GE. 3) THEN
!CDIR LISTVEC
          DO NSTART = 0, NSTARTMP-4, 4
            IP = BUFRCV(NSTART+1)+0.1
            FX(IP) = FX(IP)+BUFRCV(NSTART+2)
            FY(IP) = FY(IP)+BUFRCV(NSTART+3)
            FZ(IP) = FZ(IP)+BUFRCV(NSTART+4)
          ENDDO
      ELSE IF(IDIM .GE. 2) THEN
!CDIR LISTVEC
          DO NSTART = 0, NSTARTMP-4, 4
            IP = BUFRCV(NSTART+1)+0.1
            FX(IP) = FX(IP)+BUFRCV(NSTART+2)
            FY(IP) = FY(IP)+BUFRCV(NSTART+3)
          ENDDO
      ELSE
!CDIR LISTVEC
          DO NSTART = 0, NSTARTMP-4, 4
            IP = BUFRCV(NSTART+1)+0.1
            FX(IP) = FX(IP)+BUFRCV(NSTART+2)
          ENDDO
      ENDIF
!
!C    CALL FTRACE_REGION_END("ddcom3:300-310")
!
      IPART = IPART
!
!
      RETURN
      END
!
      SUBROUTINE DDCOMX(IPART,IDIM,LDOM,NBPDOM,NDOM,IPSLF,IPSND,MBPDOM,FX,FY,FZ,NP,IUT0,IERR,&
                        snd_desc, rcv_desc, MAXBUF)
!fj                        BUFSND, BUFRCV, MAXBUF)
! Fujitsu start 202103
      use xmp_api
      use mpi
! Fujitsu end 202103
!      INCLUDE 'mpif.h'
!      INCLUDE 'xmp_coarray.h'
      IMPLICIT REAL*4(A-H,O-Z)
!CTTDEBG
      REAL*8 DFX(NP),DFY(NP),DFZ(NP)
!CTTDEBG
      DIMENSION LDOM(NDOM),NBPDOM(NDOM),IPSLF(MBPDOM,NDOM),IPSND(MBPDOM,NDOM), FX(NP),FY(NP),FZ(NP)
! Fujitsu start 202103
!      DIMENSION BUFSND(MAXBUF)[*], BUFRCV(MAXBUF)[*]
      REAL*4 , POINTER :: BUFSND ( : ) => null ( )
      REAL*4 , POINTER :: BUFRCV ( : ) => null ( )
      INTEGER*8 :: snd_desc, rcv_desc
      INTEGER*8 :: snd_sec, rcv_sec
      INTEGER*8, DIMENSION(1) :: snd_lb, snd_ub, rcv_lb, rcv_ub
      INTEGER*8 :: st_desc
      INTEGER*8 :: st_sec
      INTEGER*8, DIMENSION(1) :: st_lb, st_ub
      INTEGER*8 :: start_pos, end_pos
      INTEGER*4 :: img_dims(1)
      INTEGER*4 status
! Fujitsu end 202103
!
      PARAMETER ( MAXDOM = 10000 )
      INTEGER*4 MSGIDS(MAXDOM),MSGSTS(MPI_STATUS_SIZE,MAXDOM)
!
      INTEGER MAX_RECV_LEN
! Fujitsu start 202103
!      INTEGER ,ALLOCATABLE :: START_R(:)[:]
      INTEGER*4 , POINTER :: START_R ( : ) => null ( )
! Fujitsu end 202103
!      INTEGER ,ALLOCATABLE :: END_R(:)[:]
      INTEGER ,ALLOCATABLE :: START_S(:)
      INTEGER ,ALLOCATABLE :: END_S(:)
      INTEGER START_RR, END_RR
!      REAL*4,  ALLOCATABLE :: BUFRCV2(:)[:]
!
      CHARACTER*60 ERMSGB  / ' ## SUBROUTINE DDCOMX: FATAL     ERROR OCCURRENCE; RETURNED' /
      CHARACTER*60 EREXP1  / ' DIMENSION SIZE OF INTERNAL      ARRAYS IS NOT SUFFICIENT  ' /
      CHARACTER*60 EREXP2  / ' DIMENSION SIZE OF PASSED BUFFER ARRAYS IS NOT SUFFICIENT  ' /
      CHARACTER*60 EREXP3  / ' RECEIVED NODE NUMBER IS OUT OF THE GLOBAL NODE NUMBER     ' /
!      PRINT *,"MAXBUF=",MAXBUF
!
!
!      EXCHANGE X, Y, AND Z RESIDUALS AMONG THE NEIGHBORING SUB-DOMAINS
!     AND SUPERIMPOSE THE EXCHANGED RESIDUALS TO THE CALLING TASK'S
!     RESIDUALS, FOR DOMAIN-DECOMPOSITION PROGRAMMING MODEL
!
!                            ( MPI VERSION )
!
!
! NOTE 1; ALL 'MPI' ROUTINES RETURN AN ERROR CODE 'IERR' WHICH INDICATES
!       THE STATUS OF ITS EXECUTION. THIS SUBROUTINE IGNORES SUCH ERROR
!       CODE AND RETURNS THE SEQUENCE TO THE CALLING PROGRAM UNIT,
!       REGARDLESS OF THE VALUE OF THE 'MPI' RETURN CODE.
!
! NOTE 2; SOME COMPILERS, SUCH AS OFORT90 IN HI-UXMPP, SUPPORT AUTOMATIC
!       PRECISION EXPANSION, WHERE ALL THE CONSTANTS, VARIABLES AND
!       ARRAYS OF 4-BYTE PRECISION (REAL*4) ARE AUTOMATICALLY CONVERTED
!       TO THOSE OF 8-BYTE PRECISION (REAL*8) WITH UNFORMATTED I/O DATA 
!       BEING KEPT AS THEY ARE (IF SO SPECIFIED). WHEN USING SUCH 
!       FEATURES (FUNCTIONS) OF A COMPILER, SPECIAL CARE IS NEEDED
!       BECAUSE A COUPLE OF MPI SUBROUTINES CALLED IN THIS SUBPROGRAM
!       ACCEPT THE DATA TYPE (DATA PRECISION) AS THEIR INPUT AND
!       PERFORM THE OPERATIONS ACCORDING TO THIS INPUT VALUE. THIS
!       INTERFACE SUPPORTS THE AUTOMATIC PRECISION EXPANSION MENTIONED
!       ABOVE. IF YOU WISH TO USE SUCH FEATURE, ADD '-DPRECEXP' OPTION
!       WHEN INVOKING 'cpp' FOR PRI-PROCESSING THIS SOURCE PROGRAM FILE.
!
!
!     ARGUMENT LISTINGS
!       (1) INPUT
! INT *4   IPART       ; SUB-DOMAIN NUMBER THAT THE CALLING TASK IS
!                       TAKING CARE OF
!           NOTES ; ARGUMENT 'IPART' IS NOT CURRENTLY USED. IT IS
!                  RETAINED FOR A POSSIBLE FUTURE USE.
! INT *4   IDIM             ; SPACE DIMENSION ( 1, 2, OR 3 )
! INT *4   LDOM      (IDOM) ; NEIGHBORING SUB-DOMAIN NUMBER
! INT *4   NBPDOM    (IDOM) ; NUMBER OF INTER-CONNECT BOUNDARY NODES
!                            SHARING WITH THE IDOM'TH NEIGHBORING
!                            SUB-DOMAIN, LDOM(IDOM)
! INT *4   NDOM             ; NUMBER OF THE NEIGHBORING SUB-DOMAINS
! INT *4   IPSLF (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            CALLING TASK'S SUB-DOMAIN, FOR THE IDOM'TH
!                            NEIGHBORING SUB-DOMAIN, LDOM(IDOM)
! INT *4   IPSND (IBP,IDOM) ; INTER-CONNECT BOUNDARY NODE NUMBER IN THE
!                            SUB-DOMAIN THAT IS RECEIVING THE CALLING
!                            TASK'S RESIDUALS.
! INT *4   MBPDOM           ; THE DIMENSION SIZE OF THE FIRST ELEMENTS
!                            OF THE PASSED ARRAYS 'IPSLF' AND 'IPSND'
!                            (I.E. THE MAXIMUM NUMBER OF THE
!                             INTER-CONNECT BOUNDARY NODES FOR A
!                             NEIGHBORING SUB-DOMAIN)
! INT *4   NP               ; NUMBER OF THE TOTAL NODES IN THE CALLING
!                            TASK'S SUB-DOMAIN
! INT *4   IUT0             ; FILE NUMBER TO WRITE ERROR MESSAGE
! INT *4   MAXBUF           ; LENGTH OF THE PASSED COMMUNICATION BUFFERS
!                            'BUFSND' AND 'BUFRCV' IN WORDS. 'MAXBUF'
!                             MUST BE NO SMALLER THAN 4 TIMES THE TOTAL
!                             NUMBER OF INTER-CONNECT BOUNDARY NODES IN
!                             THE CALLING TASK
!
!       (2) OUTPUT
! INT *4   IERR             ; RETURN CODE WHOSE VALUE WILL BE EITHER
!                   0 --- INDICATING SUCCESSFUL TERMINATION
!                OR 1 --- INDICATING OCCURRENCE OF SOME ERROR CONDITIONS
!
!       (3) INPUT-OUTPUT
! REAL*4   FX(IP)           ; X-DIRECTION RESIDUAL VECTOR
! REAL*4   FY(IP)           ; Y-DIRECTION RESIDUAL VECTOR
! REAL*4   FZ(IP)           ; Z-DIRECTION RESIDUAL VECTOR
!
!       (4) WORK
! REAL*4   BUFSND(IBUF)     ; HOLDS THE VALUES OF THE INTER-CONNECT
!                            BOUNDARY NODE NUMBER IN THE NEIGHBORING
!                            SUB-DOMAINS AND THE RESIDUALS OF THE
!                            CALLING TASK'S SUB-DOMAIN WHEN SENDING
!                            THE RESIDUALS
!                         
! REAL*4   BUFRCV(IBUF)     ; HOLDS THE VALUES OF THE INTER-CONNECT
!                            BOUNDARY NODE NUMBER IN THE CALLING TASK'S
!                            SUB-DOMAIN AND THE RESIDUALS OF THE
!                            NEIGHBORING SUB-DOMAINS AT THE RECEIPT OF
!                            THE RESIDUALS FROM THE NEIGHBORING
!                            SUB-DOMAINS
      CALL MPI_COMM_RANK(MPI_COMM_WORLD,ITASK,IERR)
! Fujitsu start 202103
!      call xmp_api_init
!
      snd_lb(1) = 1
      snd_ub(1) = MAXBUF
      rcv_lb(1) = 1
      rcv_ub(1) = MAXBUF
!      call xmp_new_coarray(snd_desc, 4, 1, snd_lb, snd_ub, 1, img_dims)
!      call xmp_new_coarray(rcv_desc, 4, 1, rcv_lb, rcv_ub, 1, img_dims)
!
      call xmp_reshape_coarray(snd_desc, snd_desc, 4, 1, &
                               snd_lb, snd_ub, 1, img_dims)
      call xmp_reshape_coarray(rcv_desc, rcv_desc, 4, 1, &
                               rcv_lb, rcv_ub, 1, img_dims)

      call xmp_coarray_bind(snd_desc,BUFSND)
      call xmp_coarray_bind(rcv_desc,BUFRCV)
!
!      allocate(START_R(1:NP)[*])
      st_lb(1) = 1
      st_ub(1) = NP
      call xmp_new_coarray(st_desc, 4, 1, st_lb, st_ub, 1, img_dims)
      call xmp_coarray_bind(st_desc,START_R)
! Fujitsu end 202103
!      allocate(END_R(1:NP)[*])
      allocate(START_S(1:NP))
      allocate(END_S(1:NP))
!      allocate(BUFRCV2(1:MAXBUF)[*])
!      IF (ITASK.eq.0) PRINT *,"CALL DDCOMX"
#ifdef USE_BARRIER
      call MPI_BARRIER(MPI_COMM_WORLD, IERR)
#endif
      IERR = 0

      IF(IDIM.EQ.0) THEN
          NSKIP=1
      ELSE IF(IDIM.EQ.1) THEN
          NSKIP=1
      ELSE IF(IDIM.EQ.2) THEN 
          NSKIP=2
      ELSE IF(IDIM.EQ.3) THEN 
          NSKIP=3
      ELSE
          WRITE(IUT0,*) EREXP1
          IERR = 1
          RETURN
      ENDIF
!
!
! CHECK THE INTERNAL ARRAY SIZE
!
!
!
      IF(2*NDOM.GT.MAXDOM) THEN
          WRITE(IUT0,*) ERMSGB
          WRITE(IUT0,*) EREXP1
          IERR = 1
          RETURN
      ENDIF
!
!
!
! POST ALL THE EXPECTED RECEIVES
!
!
!
!      PRINT *,"NDOM:",NDOM
!
! Fujitsu start 202103
!      ME=THIS_IMAGE()
      ME=xmp_this_image()
! Fujitsu end 202103
      START_DASH=0
      MAX_RECV_LEN = 0 
      NSTART = 1
      IF (NDOM > 1) START_R(LDOM(1))=NSTART
      DO 110 IDOM = 1 , NDOM
          MSGTYP = 1
          IRECV  = LDOM(IDOM)-1
          MSGLEN = NSKIP*NBPDOM(IDOM)
!          END_R(LDOM(IDOM))=START_R(LDOM(IDOM))+MSGLEN-1
!        IF(ITASK.eq.7) PRINT *,"(Recv)ITASK=",ITASK,"LDOM(",IDOM,")-1=",IRECV
          IF(NSTART+MSGLEN-1.GT.MAXBUF) THEN
              WRITE(IUT0,*) ERMSGB
              WRITE(IUT0,*) EREXP2
              IERR = 1
              RETURN
          ENDIF
      if(MAX_RECV_LEN.lt.MSGLEN)MAX_RECV_LEN=MSGLEN
!
#ifdef PRECEXP
!          CALL MPI_IRECV(BUFRCV(NSTART),MSGLEN,MPI_REAL8,IRECV,MSGTYP,MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
#else
!          CALL MPI_IRECV(BUFRCV(NSTART),MSGLEN,MPI_REAL ,IRECV,MSGTYP,MPI_COMM_WORLD,MSGIDS(IDOM),IERR)
#endif
!
!      IF (ITASK.eq.0) PRINT *,"MSGLEN=",MSGLEN ,"FROM",IRECV
!      PRINT *,"MSGLEN=",MSGLEN ,"FROM",IRECV,"TO",ITASK
          NSTART = NSTART+MSGLEN
          IF(IDOM.LT.NDOM)START_R(LDOM(IDOM+1))=NSTART
!          PRINT *,NSTART,MSGLEN
  110 CONTINUE
!     PRINT *,"MAX_RECV_LEN = ",MAX_RECV_LEN
!
      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
!
!
!
! SET UP THE SEND BUFFER
!
!
!C    CALL FTRACE_REGION_BEGIN("ddcom3:200-210")
#ifdef USE_DETAIL
      call start_collection('ddcomx_210')
#endif
!CDIR PARALLEL DO PRIVATE(NSTART,IP,IPS)
      DO 210 IDOM = 1 , NDOM
         NSTART  = 0
         DO 205 ITMP = 2 , IDOM
            NSTART = NSTART + NBPDOM(ITMP-1)*NSKIP
 205     CONTINUE
!CDIR NOINNER
         IF(IDIM.EQ.0) THEN
!ocl norecurrence(BUFSND)
            DO IBP=1,NBPDOM(IDOM)
               NSTART2 = NSTART + NSKIP * (IBP-1)
               IP      = IPSLF(IBP,IDOM)
               IPS     = IPSND(IBP,IDOM)
               BUFSND(NSTART2+1) = IPS
            ENDDO
         ELSE IF(IDIM.EQ.1) THEN
!ocl norecurrence(BUFSND)
            DO IBP=1,NBPDOM(IDOM)
               NSTART2 = NSTART + NSKIP * (IBP-1)
               IP      = IPSLF(IBP,IDOM)
               BUFSND(NSTART2+1) = FX(IP)
            ENDDO
         ELSE IF(IDIM.EQ.2) THEN
!ocl norecurrence(BUFSND)
            DO IBP=1,NBPDOM(IDOM)
               NSTART2 = NSTART + NSKIP * (IBP-1)
               IP      = IPSLF(IBP,IDOM)
               BUFSND(NSTART2+1) = FX(IP)
               BUFSND(NSTART2+2) = FY(IP)
            ENDDO
         ELSE IF(IDIM.EQ.3) THEN
!ocl norecurrence(BUFSND)
            DO IBP=1,NBPDOM(IDOM)
               NSTART2 = NSTART + NSKIP * (IBP-1)
               IP      = IPSLF(IBP,IDOM)
               BUFSND(NSTART2+1) = FX(IP)
               BUFSND(NSTART2+2) = FY(IP)
               BUFSND(NSTART2+3) = FZ(IP)
            ENDDO
         ENDIF
 210  CONTINUE
#ifdef USE_DETAIL
      call stop_collection('ddcomx_210')
#endif
      
!
!
!
! SEND THE RESIDUALS
!
!
!
      NSTART = 1
      IF (NDOM > 1) START_S(LDOM(1))=NSTART
      DO 220 IDOM = 1 , NDOM
          MSGTYP = 1
          ISEND  = LDOM(IDOM)-1
          MSGLEN = NSKIP*NBPDOM(IDOM)
          END_S(LDOM(IDOM))=START_S(LDOM(IDOM))+MSGLEN-1
!        IF(ITASK.eq.7) PRINT *,"(Send)ITASK=",ITASK,"LDOM(",IDOM,")-1=",ISEND

#ifdef PRECEXP
!          CALL MPI_ISEND(BUFSND(NSTART),MSGLEN,MPI_REAL8,ISEND,MSGTYP,MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
#else
!          CALL MPI_ISEND(BUFSND(NSTART),MSGLEN,MPI_REAL ,ISEND,MSGTYP,MPI_COMM_WORLD,MSGIDS(NDOM+IDOM),IERR)
#endif
!
!      IF (ITASK.eq.0) PRINT *,"MSGLEN=",MSGLEN ,"TO",ISEND
!      PRINT *,"FROM",ITASK,"MSGLEN=",MSGLEN ,"TO",ISEND
          NSTART = NSTART+MSGLEN 
          IF(IDOM.LT.NDOM)START_S(LDOM(IDOM+1))=NSTART
!          PRINT *,NSTART,MSGLEN
  220 CONTINUE
!
!
! Fujitsu start 202103
!     SYNC ALL
      call xmp_sync_all(status)
!
      call xmp_new_array_section(snd_sec,1)
      call xmp_new_array_section(rcv_sec,1)
      call xmp_new_array_section(st_sec,1)
! Fujitsu start 202103
!
     DO IDOM = 1, NDOM
!     PRINT *,ME,"->",LDOM(IDOM)," BUFRECV(",START_R(ME)[LDOM(IDOM)],":",END_R(ME)[LDOM(IDOM)],")[",LDOM(IDOM),"]=BUFSND(",START_S(LDOM(IDOM)),":",END_S(LDOM(IDOM)),")"
!        BUFRCV(START_R(ME)[LDOM(IDOM)]:END_R(ME)[LDOM(IDOM)])[LDOM(IDOM)] = &
!             BUFSND(START_S(LDOM(IDOM)):END_S(LDOM(IDOM)))
! Fujitsu start 202103
!        START_RR = START_R(ME)[LDOM(IDOM)]
        start_pos = ME
        end_pos = ME
        call xmp_array_section_set_triplet(st_sec,1, &
                                start_pos,end_pos,1,status)
        img_dims(1) = LDOM(IDOM)
        call xmp_coarray_get_scalar(img_dims,st_desc,st_sec, &
                             START_RR,status);
! Fujitsu end 202103
        END_RR = START_RR + (END_S(LDOM(IDOM)) - START_S(LDOM(IDOM)))
! Fujitsu start 202103
!        BUFRCV(START_RR:END_RR)[LDOM(IDOM)] = &
!             BUFSND(START_S(LDOM(IDOM)):END_S(LDOM(IDOM)))
        start_pos = START_RR
        end_pos = END_RR
        call xmp_array_section_set_triplet(rcv_sec,1, &
                                    start_pos,end_pos,1,status)
        start_pos = START_S(LDOM(IDOM))
        end_pos = END_S(LDOM(IDOM))
        call xmp_array_section_set_triplet(snd_sec,1, &
                                    start_pos,end_pos,1,status)
        img_dims = LDOM(IDOM)
        call xmp_coarray_put(img_dims,rcv_desc,rcv_sec, &
                             snd_desc,snd_sec,status);
! Fujitsu end 202103
     END DO

! Fujitsu start 202103
!     SYNC ALL
      call xmp_sync_all(status)
! Fujitsu end 202103

!        SYNC ALL
!       DO 231 IDOM = 1 , NDOM
! !      SYNC ALL 
! !      PRINT *,ME,"->",LDOM(IDOM)," BUFRECV(",START_R(LDOM(IDOM)),":",END_R(LDOM(IDOM)),")=BUFSND(",START_S(ME)[LDOM(IDOM)],":",END_S(ME)[LDOM(IDOM)],")[",LDOM(IDOM),"]"
!       BUFRCV(START_R(LDOM(IDOM)):END_R(LDOM(IDOM))) = &
!            BUFSND(START_S(ME)[LDOM(IDOM)]:END_S(ME)[LDOM(IDOM)])[LDOM(IDOM)]
! !      SYNC ALL 
!   231 CONTINUE
!        SYNC ALL


!     IF(ITASK.eq.0)THEN
!     DO 240 II=1,MAXBUF
!      IF(.not.BUFRCV(II).eq.BUFRCV2(II)) PRINT *,II,BUFRCV(II),BUFRCV2(II),BUFRCV(II).eq.BUFRCV2(II)
!  240 CONTINUE
!     END IF
!      CALL MPI_BARRIER(MPI_COMM_WORLD,IERR)
! WAIT FOR THE COMPLETION OF ALL THE REQUESTED COMMUNICATIONS
!
!
!
!      CALL MPI_WAITALL(2*NDOM,MSGIDS,MSGSTS,IERR)
! 
! 
! IMPORTANT NOTES!
!        AFTER A NON-BLOCKING SEND/RECEIVE ROUTINE, SUCH AS 'MPI_ISEND'
!    OR 'MPI_IRECV', IS CALLED, THE COMMUNICATION REQUEST CREATED BY
!    THESE ROUTINES MUST BE FREED EITHER BY EXPLICITLY OR IMPLICITLY.
!   'MPI_REQUEST_FREE' FREES SUCH REQUEST EXPLICITLY, WHILE A ROUTINE
!    WHICH IDENTIFIES COMPLETION OF THE REQUEST, SUCH AS 'MPI_WAIT',
!    'MPI_WAITANY', OR 'MPI_WAITALL' IMPLICITLY FREES THE REQUEST.
!        THIS INTERFACE PROGRAM USES 'MPI_WAITALL' ROUTINES TO FREE SUCH
!    REQUESTS. PAY PARTICULAR ATTENTION IF YOU WISH TO, INSTEAD, USE
!   'MPI_REQUEST_FREE', BECAUSE 'MPI_REQUEST_FREE' FREES THE REQUESTS
!    REGARDLESS OF THE STATE OF THE PREVIOUSLY CALLED COMMUNICATION
!    ROUTINES, THUS SOMETIMES FREES REQUESTS WHICH HAVE NOT BEEN
!    COMPLETED.
!
!CTTDEBG
#ifdef USE_DETAIL
      call start_collection('ddcomx_A')
#endif
!ocl simd
!ocl swp
!ocl xfill      
!ocl loop_nofusion      
      DO IP=1, NP
          DFX(IP)=DBLE(FX(IP))
      ENDDO
!ocl simd
!ocl swp
!ocl xfill      
!ocl loop_nofusion      
      DO IP=1, NP
          DFY(IP)=DBLE(FY(IP))
      ENDDO
!ocl simd
!ocl swp
!ocl xfill      
      DO IP=1, NP
          DFZ(IP)=DBLE(FZ(IP))
      ENDDO
#ifdef USE_DETAIL
      call stop_collection('ddcomx_A')
#endif
!CTTDEBG END
!
! SUPERIMPOSE THE RECEIVED RESIDUALS
!
!
      NSTART = 0
#ifdef USE_DETAIL
      call start_collection('ddcomx_B')
#endif
      DO IDOM = 1 , NDOM
         IF(IDIM .EQ. 0) THEN
            DO IBP = 1, NBPDOM(IDOM)
               NSTART2 = NSTART + NSKIP * (IBP-1)
               IPSND(IBP,IDOM)=BUFRCV(NSTART2+1)+0.1
            ENDDO
            NSTART = NSTART + NSKIP * NBPDOM(IDOM)
            
         ELSE IF(IDIM .EQ. 1) THEN
!ocl norecurrence(DFX)
            DO IBP = 1, NBPDOM(IDOM)
               IP = IPSND(IBP,IDOM)
               IF(IP.LT.1 .OR. IP.GT.NP) THEN
                  IERR = 1
               ENDIF
               NSTART2 = NSTART + NSKIP * (IBP-1)
               DFX(IP) = DFX(IP) + DBLE(BUFRCV(NSTART2+1))
            ENDDO
            NSTART = NSTART + NSKIP * NBPDOM(IDOM)
            
         ELSE IF(IDIM .EQ. 2) THEN
!ocl norecurrence(DFX,DFY)
            DO IBP = 1, NBPDOM(IDOM)
               IP = IPSND(IBP,IDOM)
               IF(IP.LT.1 .OR. IP.GT.NP) THEN
                  IERR = 1
               ENDIF
               NSTART2 = NSTART + NSKIP * (IBP-1)
               DFX(IP) = DFX(IP) + DBLE(BUFRCV(NSTART2+1))
               DFY(IP) = DFY(IP) + DBLE(BUFRCV(NSTART2+2))
            ENDDO
            NSTART = NSTART + NSKIP * NBPDOM(IDOM)
            
         ELSE IF(IDIM .EQ. 3) THEN
!ocl norecurrence(DFX,DFY,DFZ)
            DO IBP = 1, NBPDOM(IDOM)
               IP = IPSND(IBP,IDOM)
               IF(IP.LT.1 .OR. IP.GT.NP) THEN
                  IERR = 1
               ENDIF
               NSTART2 = NSTART + NSKIP * (IBP-1)
               DFX(IP) = DFX(IP) + DBLE(BUFRCV(NSTART2+1))
               DFY(IP) = DFY(IP) + DBLE(BUFRCV(NSTART2+2))
               DFZ(IP) = DFZ(IP) + DBLE(BUFRCV(NSTART2+3))
            ENDDO
            NSTART = NSTART + NSKIP * NBPDOM(IDOM)
         ENDIF
!CTTDEBG END
!
!          IF(IDIM.EQ.1) THEN
!            FX(IP) = FX(IP)+BUFRCV(NSTART+1)
!          ELSE IF(IDIM .EQ. 2) THEN 
!            FX(IP) = FX(IP)+BUFRCV(NSTART+1)
!            FY(IP) = FY(IP)+BUFRCV(NSTART+2)
!          ELSE IF(IDIM .EQ. 3) THEN 
!            FX(IP) = FX(IP)+BUFRCV(NSTART+1)
!            FY(IP) = FY(IP)+BUFRCV(NSTART+2)
!            FZ(IP) = FZ(IP)+BUFRCV(NSTART+3)
!          ENDIF
      ENDDO
#ifdef USE_DETAIL
      call stop_collection('ddcomx_B')
#endif
!
!CTTDEBG
#ifdef USE_DETAIL
      call start_collection('ddcomx_C')
#endif
      IF(IDIM.EQ.1) THEN
!ocl simd
!ocl swp
         DO IP=1,NP
            FX(IP) = SNGL(DFX(IP))
         ENDDO
      ELSE IF(IDIM.EQ.2) THEN
!ocl simd
!ocl swp
!ocl loop_nofusion         
         DO IP=1,NP
            FX(IP) = SNGL(DFX(IP))
         ENDDO
!ocl simd
!ocl swp
         DO IP=1,NP
            FY(IP) = SNGL(DFY(IP))
         ENDDO
      ELSE IF(IDIM.EQ.3) THEN
!ocl simd
!ocl swp
!ocl loop_nofusion         
         DO IP=1,NP
            FX(IP) = SNGL(DFX(IP))
         ENDDO
!ocl simd
!ocl swp
!ocl loop_nofusion         
         DO IP=1,NP
            FY(IP) = SNGL(DFY(IP))
         ENDDO
!ocl simd
!ocl swp
         DO IP=1,NP
            FZ(IP) = SNGL(DFZ(IP))
         ENDDO
      ENDIF
#ifdef USE_DETAIL
      call stop_collection('ddcomx_C')
#endif
!CTTDEBG
!
! Fujitsu start 202103
      call xmp_free_array_section(snd_sec)
      call xmp_free_array_section(rcv_sec)
      call xmp_free_array_section(st_sec)
!
      call xmp_coarray_deallocate(st_desc, status)
! Fujitsu end 202103
!
#ifdef USE_BARRIER      
      call MPI_BARRIER(MPI_COMM_WORLD, IERR)
#endif
      IF(IERR .eq. 1) THEN
        WRITE(IUT0,*) ERMSGB
        WRITE(IUT0,*) EREXP3
        RETURN
      ENDIF
!
!     IPART = IPART
      RETURN
      END
!
!CTT SUBROUTINE DDCOMX END
